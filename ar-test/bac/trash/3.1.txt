import SwiftUI
import ARKit
import SceneKit
import AVFoundation

class ARCoordinator: NSObject, ARSCNViewDelegate, ARSessionDelegate {
    var parent: ContentView
    var arView: ARSCNView?
    var depthImageView: UIImageView?
    var lastFrameTimestamp: TimeInterval?

    init(parent: ContentView) {
        self.parent = parent
    }

    func session(_ session: ARSession, didUpdate frame: ARFrame) {
        let pixelBuffer = frame.capturedImage
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        let context = CIContext()
        if let cgImage = context.createCGImage(ciImage, from: ciImage.extent) {
            let uiImage = UIImage(cgImage: cgImage)
            DispatchQueue.main.async {
                self.depthImageView?.image = uiImage
            }

            // [fps]
            let currentTime = frame.timestamp
            if let lastTime = lastFrameTimestamp {
                let deltaTime = currentTime - lastTime
                let fps = 1.0 / deltaTime
                // print("Current FPS: \(fps)")
            }
            lastFrameTimestamp = currentTime
        }

        if let sceneDepth = frame.sceneDepth?.depthMap {
            displayDepthMap(sceneDepth)
        }
    }

    private func displayDepthMap(_ depthMap: CVPixelBuffer) {
        CVPixelBufferLockBaseAddress(depthMap, .readOnly)
        defer { CVPixelBufferUnlockBaseAddress(depthMap, .readOnly) }

        let width = CVPixelBufferGetWidth(depthMap)
        let height = CVPixelBufferGetHeight(depthMap)
        let baseAddress = CVPixelBufferGetBaseAddress(depthMap)!

        // Assuming the depth map is in Float32 format
        let floatBuffer = baseAddress.assumingMemoryBound(to: Float32.self)

        // Convert depth data to UIImage for display
        let depthImage = imageFromDepthMap(floatBuffer, width: width, height: height)

        // Update the depth image view on the main thread
        DispatchQueue.main.async {
            self.depthImageView?.image = depthImage
        }
    }

    private func imageFromDepthMap(_ depthMap: UnsafePointer<Float32>, width: Int, height: Int) -> UIImage {
        // Convert depth map to grayscale image
        let bitsPerComponent = 8
        let bytesPerRow = width
        var pixelData = [UInt8](repeating: 0, count: width * height)
        for y in 0..<height {
            for x in 0..<width {
                let depthValue = depthMap[y * width + x]
                let pixelValue = UInt8(min(255.0, depthValue * 255.0)) // Normalize to 0-255
                pixelData[y * width + x] = pixelValue
            }
        }
        let colorSpace = CGColorSpaceCreateDeviceGray()
        let bitmapInfo = CGBitmapInfo(rawValue: CGImageAlphaInfo.none.rawValue)
        let provider = CGDataProvider(data: NSData(bytes: &pixelData, length: pixelData.count))
        let cgImage = CGImage(width: width, height: height, bitsPerComponent: bitsPerComponent, bitsPerPixel: bitsPerComponent, bytesPerRow: bytesPerRow, space: colorSpace, bitmapInfo: bitmapInfo, provider: provider!, decode: nil, shouldInterpolate: false, intent: .defaultIntent)

        // Correct the orientation and aspect ratio
        let uiImage = UIImage(cgImage: cgImage!)
        let rotatedImage = uiImage.rotate(radians: .pi / 2) // Rotate 90 degrees
        return rotatedImage
    }
}


class AVCaptureCoordinator: NSObject, AVCaptureVideoDataOutputSampleBufferDelegate {
    var parent: ContentView
    var depthImageView: UIImageView?
    var captureSession: AVCaptureSession

    init(parent: ContentView) {
        self.parent = parent
        self.captureSession = AVCaptureSession()
        super.init()
        setupCaptureSession()
    }

    private func setupCaptureSession() {
        captureSession.beginConfiguration()
        guard let videoDevice = AVCaptureDevice.default(.builtInUltraWideCamera, for: .video, position: .back) else { return }
        guard let videoInput = try? AVCaptureDeviceInput(device: videoDevice) else { return }
        if captureSession.canAddInput(videoInput) {
            captureSession.addInput(videoInput)
        }

        let videoOutput = AVCaptureVideoDataOutput()
        videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "videoQueue"))
        if captureSession.canAddOutput(videoOutput) {
            captureSession.addOutput(videoOutput)
        }
        captureSession.commitConfiguration()
    }

    func startCaptureSession() {
        captureSession.startRunning()
    }

    func stopCaptureSession() {
        captureSession.stopRunning()
    }

    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        let context = CIContext()
        if let cgImage = context.createCGImage(ciImage, from: ciImage.extent) {
            let uiImage = UIImage(cgImage: cgImage)
            DispatchQueue.main.async {
                self.depthImageView?.image = uiImage
            }
        }
    }
}


struct ContentView: UIViewRepresentable {
    @State private var depthImage: UIImage?

    func makeUIView(context: Context) -> UIView {
        let containerView = UIView(frame: .zero)
        
        // [wide]
        let previewLayer = AVCaptureVideoPreviewLayer(session: context.avCaptureCoordinator.captureSession)
        previewLayer.videoGravity = .resizeAspectFill
        previewLayer.frame = containerView.bounds
        containerView.layer.addSublayer(previewLayer)

        // [ar]
        let arView = ARSCNView(frame: .zero)
        arView.translatesAutoresizingMaskIntoConstraints = false
        containerView.addSubview(arView)

        NSLayoutConstraint.activate([
            arView.topAnchor.constraint(equalTo: containerView.topAnchor),
            arView.bottomAnchor.constraint(equalTo: containerView.bottomAnchor),
            arView.leadingAnchor.constraint(equalTo: containerView.leadingAnchor),
            arView.trailingAnchor.constraint(equalTo: containerView.trailingAnchor)
        ])

        // [depth]
        let depthImageView = UIImageView(frame: .zero)
        depthImageView.translatesAutoresizingMaskIntoConstraints = false
        depthImageView.contentMode = .scaleAspectFit
        containerView.addSubview(depthImageView)

        NSLayoutConstraint.activate([
            depthImageView.topAnchor.constraint(equalTo: containerView.topAnchor, constant: 20),
            depthImageView.leadingAnchor.constraint(equalTo: containerView.leadingAnchor, constant: 20),
            depthImageView.widthAnchor.constraint(equalToConstant: 200),
            depthImageView.heightAnchor.constraint(equalToConstant: 200)
        ])

        context.arCoordinator.depthImageView = depthImageView
        context.avCaptureCoordinator.depthImageView = depthImageView

        // [button]
        let saveButton = UIButton(type: .system)
        saveButton.setTitle("Save Images", for: .normal)
        saveButton.translatesAutoresizingMaskIntoConstraints = false
        saveButton.addTarget(context.avCaptureCoordinator, action: #selector(AVCaptureCoordinator.saveImages), for: .touchUpInside)
        containerView.addSubview(saveButton)

        NSLayoutConstraint.activate([
            saveButton.bottomAnchor.constraint(equalTo: containerView.bottomAnchor, constant: -20),
            saveButton.centerXAnchor.constraint(equalTo: containerView.centerXAnchor)
        ])

        // [ar conf]
        let configuration = ARWorldTrackingConfiguration()
        configuration.planeDetection = [.horizontal, .vertical]

        // Enable scene depth
        if type(of: configuration).supportsFrameSemantics(.sceneDepth) {
            configuration.frameSemantics.insert(.sceneDepth)
        }

        arView.session.run(configuration)
        arView.delegate = context.arCoordinator
        arView.session.delegate = context.arCoordinator
        context.arCoordinator.arView = arView

        // [sessions]
        context.avCaptureCoordinator.startCaptureSession()
        return containerView
    }

    func updateUIView(_ uiView: UIView, context: Context) {
        // Update the view if needed
    }

    func makeCoordinator() -> Coordinator {
        Coordinator(self)
    }

    class Coordinator {
        var parent: ContentView
        var arCoordinator: ARCoordinator
        var avCaptureCoordinator: AVCaptureCoordinator

        init(_ parent: ContentView) {
            self.parent = parent
            self.arCoordinator = ARCoordinator(parent: parent)
            self.avCaptureCoordinator = AVCaptureCoordinator(parent: parent)
        }
    }
}

extension UIImage {
    func rotate(radians: CGFloat) -> UIImage {
        let rotatedSize = CGRect(origin: .zero, size: size)
            .applying(CGAffineTransform(rotationAngle: radians))
            .integral.size
        UIGraphicsBeginImageContext(rotatedSize)
        let context = UIGraphicsGetCurrentContext()!
        context.translateBy(x: rotatedSize.width / 2, y: rotatedSize.height / 2)
        context.rotate(by: radians)
        draw(in: CGRect(x: -size.width / 2, y: -size.height / 2, width: size.width, height: size.height))
        let rotatedImage = UIGraphicsGetImageFromCurrentImageContext()
        UIGraphicsEndImageContext()
        return rotatedImage!
    }
}

extension UIImage {
    convenience init?(pixelBuffer: CVPixelBuffer) {
        var ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        let context = CIContext()
        guard let cgImage = context.createCGImage(ciImage, from: ciImage.extent) else { return nil }
        self.init(cgImage: cgImage)
    }
}

struct ContentView_Previews: PreviewProvider {
    static var previews: some View {
        ContentView()
    }
}

